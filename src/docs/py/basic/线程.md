::: tip
在 Python 中进程和线程都是用于多任务处理的重要概念。它们允许程序同时执行多个任务，但是会有一些区别。

- 进程：进程是操作系统分配资源的基本单位，每个进程都有自己独立的内存空间和系统资源。进程之间相互独立，数据不互通，如果想要数据互通需要利用 （Inter-Process Communication，IPC）机制。如管道、队列。

- 线程：线程是进程内的执行单元，因此进程中的线程是数据共享的，但是需要注意线程安全问题，比如可以使用锁机制。

总结：进程适合需要隔离和并行执行的任务，线程适合数据共享的任务。**需要注意的是由于 Python 中的全局解释锁（Global Interpreter Lock，GIL）限制，多线程在 CPU 密集型任务中可能无法实现真正的并行执行，但对于 I/O 密集型任务是可以的。如果的确需要并行可以考虑使用多进程。**
:::

## 进程

创建进程的方式是通过 `multiprocessing` 模块下的 `Process`，该方法接受两个参数，一个新进程需要运行的函数和一个可选参数（元祖类型，最少两个）。

- `start`: 启动进程
- `join`: 等待进程执行完成

`current_process` 则是获取当前正在运行的进程信息。

```py
from multiprocessing import Process, current_process
import os

def task(name):
    process = current_process()
    # 步骤一：子进程的 id: 63888, 子进程父 id: 63886
    print(f"子进程的 id: {process.pid}, 子进程父 id: {os.getppid()}")

if __name__ == "__main__":
    p = Process(target=task, args=("子进程",))
    p.start()
    p.join()
    # 步骤二：主进程 id, 63886
    print(f"主进程 id, {os.getpid()}")
```

### join 方法

此方法是等待被开启的进程执行完毕，再往下执行。示例如下：

```py
from multiprocessing import Process
import time

def task():
    print("任务开始执行。。。")
    time.sleep(1)
    print("任务执行结束。。。")

if __name__ == "__main__":
    p = Process(target=task)
    p.start()

    print("主进程执行完毕")

# 可以看出打印结果是先执行主进程，然后再去打印子进程的代码
# 主进程执行完毕
# 任务开始执行。。。
# 任务执行结束。。。

# 如果在 start 后加入 join 方法打印结果则是
# 任务开始执行。。。
# 任务执行结束。。。
# 主进程执行完毕
```

### 僵尸进程 Zombie Process

子进程死后还有有一些资源占用（进程号，进程运行状态，进程运行时间等），等待父进程调用系统进行资源回收，除了 `init`（系统初始化进程） 进程之外，所有的进程最后都会步入僵尸进程。

危害：子进程退出后，父进程没有及时处理，僵尸进程就会占用系统资源，如果产生了大量的僵尸进程，系统资源占用过度，系统没有可用的进程号，导致系统不能产生新的进程。

### 孤儿进程 Orphan Process

孤儿进程值父进程意外或被提前终止，导致子进程成为孤儿，没有父进程，当一个进程成为孤儿进程时，会被 `init` 进程接受，成为 `init` 的子进程。由 `init` 进程负责孤儿进程的资源清理和回收，保证其能够正常被终止。

### 守护进程 Daemon Process

守护进程是后台运行的进程，守护进程的目的是执行系统服务，定时任务等，通常在系统启动式就启动并持续运行。

```py
from multiprocessing import Process
import time

def task():
    print("任务开始...")
    time.sleep(3)
    print("任务结束...")

if __name__ == "__main__":
    p = Process(target=task)
    # daemo 开启守护进程
    p.daemon = True
    p.start()

    print("主进程执行完毕...")

# 当守护进程开启后，只要主进程执行完毕，子进程也会跟着结束。所以后续的内容就不会再被打印
# 主进程执行完毕...
```

### 并发 Concurrency 、并行 Parallelism

- 并发：多个进程或者线程在同一时间段内执行，并且相互交替执行，并发并不意味着一起执行，因为多个进程和线程可能在单个处理器上交替执行。
- 并行：多个进程或者线程在同一时间内执行，需要用到处理器的多核能力，并行可以显著提高程序的运行速度

### 互斥锁

当多个进程操作同一份数据的时候，就会引发数据错乱的问题，解决的方法就是加锁处理，**锁的原理就是把并发变成串行，虽然牺牲了效率，但是保证了数据的安全性**

假设有一个抢票平台，目前余票剩余两张，但是有 8 个用户去抢这个余票，代码可能是如下这样：

::: code-group

```py [main.py]
from multiprocessing import Process
import time
import random
import json

# 查询余票
def query_ticket(name):
    with open("./tickets.json", "r", encoding="utf8") as f:
        dic = json.load(f)
        print(f"{name} 查询余票数量为 {dic['ticket']}")

def buy_ticket(name):
    with open("./tickets.json", "r", encoding="utf8") as f:
        dic = json.load(f)

    # 模拟网络延迟
    time.sleep(random.randint(1, 3))
    if dic["ticket"] > 0:
        dic["ticket"] -= 1
        with open("./tickets.json", "w", encoding="utf-8") as f:
            json.dump(dic, f)
        print(f"{name} 购买一张余票，余票数量为 {dic['ticket']}")
    else:
        print(f"余票不足，{name}买票失败")

def common(name):
    query_ticket(name)
    buy_ticket(name)

if __name__ == "__main__":
    for i in range(1, 9):
        p = Process(target=common, args=(f"用户{i} ",))
        p.start()

```

```json [tickets.json]
{ "ticket": 2 }
```

:::

代码执行后，可以看出所有的用户都能买到票。并且余票是 1。原因就是多进程执行时都触发了购买，读到了同一个数据源，导致了数据的错乱，解决的办法就是加一把锁，谁先抢到这把锁，谁就先开始买票，买票之后再把锁释放，让其余的进程继续抢这个锁。修复后的代码如下

```py
from multiprocessing import Process, Lock, set_start_method
import json
import time
import random

def query_ticket(name):
    with open("./tickets.json", "r", encoding="utf-8") as f:
        dic = json.load(f)

    print(f"{name} 查询余票，余票数量为：{dic['ticket_num']}")

def buy_ticket(name):
    with open("./tickets.json", "r", encoding="utf-8") as f:
        dic = json.load(f)

    # 模拟网络延迟
    time.sleep(random.randint(1, 5))
    if dic["ticket_num"] > 0:
        dic["ticket_num"] -= 1
        with open("./tickets.json", "w", encoding="utf-8") as f:
            json.dump(dic, f)
        print(f"{name} 购买成功，余票数量为：{dic['ticket_num']}")
    else:
        print(f"{name} 购票失败，余票不足")

def task(name, mutex):
    query_ticket(name)

    # 给购买的过程加锁
    mutex.acquire()
    buy_ticket(name)
    # 购买之后释放锁
    mutex.release()

if __name__ == "__main__":
    # mac 平台需要次代码不然会报错
    set_start_method('fork')
    mutex = Lock()
    for i in range(1, 9):
        p = Process(target=task, args=(f"用户{i} ", mutex))
        p.start()
```

为什么 `mac` 要加 `set_start_method('fork')` 原因如下：

::: warning
python3.4 更新后，默认用“spawn”，开启进程，我们要主动指定为“fork”

- spawn：使用此方式启动的进程，只会执行和 target 参数或者 run() 方法相关的代码。Windows 平台只能使用此方法，事实上该平台默认使用的也是该启动方式。相比其他两种方式，此方式启动进程的效率最低。
- fork：使用此方式启动的进程，基本等同于主进程（即主进程拥有的资源，该子进程全都有）。因此，该子进程会从创建位置起，和主进程一样执行程序中的代码。注意，此启动方式仅适用于 UNIX 平台，os.fork() 创建的进程就是采用此方式启动的。
- forserver：使用此方式，程序将会启动一个服务器进程。即当程序每次请求启动新进程时，父进程都会连接到该服务器进程，请求由服务器进程来创建新进程。通过这种方式启动的进程不需要从父进程继承资源。注意，此启动方式只在 UNIX 平台上有效。

:::

### 队列 Queue

队列的特性就是先进先出，当然 Py 中还有先进后出的队列使用 `from queue import LifoQueue` 方法与该队列基本一致。

```py
from multiprocessing import Queue

if __name__ == "__main__":
    queue = Queue(maxsize=3)
    queue.put(1)
    queue.put(2)
    queue.put(3)
    # 当超过队列长度时，会阻塞程序的运行，可以传入timeout 参数或者使用 put_nowait 方法
    # queue.put_nowait(4)
    # queue.put(4, timeout=3)

    print(queue.get())
    print(queue.get())
    print(queue.get())
    # 当取值超过队列长度时仍然会阻塞程序的执行, 可以传入 timeout 参数或者使用 get_nowait 方法  # noqa: E501
    # print(queue.get(timeout=2))
    # queue.get_nowait()

    # 判断是否为空队列
    print(queue.empty())
    # 判断队列是否是满的
    print(queue.full())
```

### 进程通信 IPC

IPC 是 Inter-Process Communication（进程间通信）的缩写，它是一种允许不同进程之间进行数据交换和通信的机制。在计算机科学和操作系统领域，IPC 是一种重要的概念，它允许多个进程在同一台计算机上协同工作、共享资源和协调任务。

```py
from multiprocessing import Process, Queue, current_process, set_start_method

def task1(q):
    process = current_process()
    q.put(f"子进程 {process.pid}存的数据")

def task2(q):
    process = current_process()
    # 子进程 84474 取数据：子进程 84473存的数据
    print(f"子进程 {process.pid} 取数据：{q.get()}")

if __name__ == "__main__":
    set_start_method("fork")
    queue = Queue()
    p1 = Process(target=task1, args=(queue,))
    p2 = Process(target=task2, args=(queue,))
    # 开启两个子进程
    p1.start()
    p2.start()

```

![process1](/py/process1.png)

#### 生产者消费者模型

生产者：负责生产和制造数据，消费者：负责消费和处理数据。通过以下代码创建出两个生产者（厨子）和两个消费者（吃货），但是还存在一些问题。比如消费者的内部是一个 `while` 循环，当消费者消费完生产者的数据时，如何跳过这个循环？

```py
from multiprocessing import Process, Queue, set_start_method
import time
import random


def producer(name, food, q):
    for i in range(10):
        # 模拟生产时间
        time.sleep(random.randint(1, 3))
        q.put(f"{food}{i}")
        print(f"{name} 生产了 {food}{i}")

def consumer(name, q):
    while True:
        # 模拟消费时间
        time.sleep(random.randint(1, 3))
        food = q.get()
        print(f"{name} 消费了 {food}")

if __name__ == "__main__":
    set_start_method("fork")
    queue = Queue()
    p1 = Process(target=producer, args=("神厨小福贵", "沙琪玛", queue))
    p2 = Process(target=producer, args=("中华小当家", "麻婆豆腐", queue))
    c1 = Process(target=consumer, args=("八戒", queue))
    c2 = Process(target=consumer, args=("福迪", queue))

    p1.start()
    p2.start()
    c1.start()
    c2.start()
```

如果想要跳出这个循环，我们可以在队列中增加两条数据作为标识。如下：

```py
# 改造消费者代码
def consumer(name, q):
    while True:
        # 模拟消费时间
        time.sleep(random.randint(1, 3))
        data = q.get()
        print(f"{name} 消费了 {data}")
        if data == "厨子下班了":
            break

if __name__ == "__main__":
    # ...

    # 等到 p1, p2 执行完毕后，在往队列中 put 两条标识数据
    p1.join()
    p2.join()
    queue.put("厨子下班了")
    queue.put("厨子下班了")

```

但还是要考虑一个问题，如果有 n 个生产者，那代码写的太啰嗦了（当然你可能会通过元类或者继承的方式，来统计消费者的个数，然后去循环添加）。

解决方案可以使用另一个对象 `JoinableQueue`，相比于 `Queue` 的基础之上增加了一个计数器的功能，每 `put` 一个数据计数器就 +1，每调用一个

`task_done` 计数器就 -1。当计数器为 0 时会执行 `q.join()` 后面的代码。

```py
from multiprocessing import Process, set_start_method, JoinableQueue
import time
import random

def producer(name, food, q):
    for i in range(10):
        # 模拟生产时间
        time.sleep(random.randint(1, 3))
        q.put(f"{food}{i}")
        print(f"{name} 生产了 {food}{i}")

def consumer(name, q):
    while True:
        # 模拟消费时间
        time.sleep(random.randint(1, 3))
        data = q.get()
        print(f"{name} 消费了 {data}")
        # 告诉队列已经取出了一条数据，并且处理完毕
        q.task_done()

if __name__ == "__main__":
    set_start_method("fork")
    queue = JoinableQueue()
    p1 = Process(target=producer, args=("神厨小福贵", "沙琪玛", queue))
    p2 = Process(target=producer, args=("中华小当家", "麻婆豆腐", queue))
    c1 = Process(target=consumer, args=("八戒", queue))
    c2 = Process(target=consumer, args=("福迪", queue))

    p1.start()
    p2.start()

    # 设置子进程为主进程的守护进程
    c1.daemon = True
    c2.daemon = True
    c1.start()
    c2.start()

    # 防止生产者数据生产慢计数器为 0 的情况提前发生，join 还是需要保留
    p1.join()
    p2.join()

    # 执行到这里时，主进程已经执行完毕，队列也已经是空的，此时消费者已经没有了存在的意义
    # 并且主进程已经结束，可以讲子进程设置成守护进程，即可实现子进程的退出
    queue.join()  # 等待队列中所有数据被取完

```

## 线程

在线程学习开始之前，再次科普：进程是资源单位，每次开设进程都要向系统申请内存空间，线程则是执行单位，每个进程中肯定会包含一个主线程。举个例子：进程好比是一个工厂，那线程就是这个工厂中的流水线。

线程的创建方式与进程基本一致，在 `Thread` 类中的方法基本与 `Process` 方法名称基本一致。

```py
from multiprocessing import Process
from threading import Thread
import time

def task():
    print("任务开始...")
    time.sleep(3)
    print("任务结束...")

if __name__ == "__main__":
    # t = Process(target=task)
    t = Thread(target=task)
    t.start()
    # t.join()
    print("主线程结束")

# 进程打印顺序为：
# 主线程结束
# 任务开始...
# 任务结束...

# 线程打印方式为：
# 任务开始...
# 主线程结束
# 任务结束...

# 原因就是开辟新的线程消耗的资源是相当小的
```

那现在有一个需求就是等到子线程的程序执行完毕之后，再去执行主线程，有什么方案呢？同样的线程也提供了 `join` 方法。用来等待子线程执行完毕。

第二种创建线程的方式就是继承，类中需要包含 `run` 方法。`Thread` 会默认调用这个方法。

```py
from threading import Thread
import time

class MyThread(Thread):
    def __init__(self, name):
        # 调用父类的构造方法
        super().__init__()
        self.name = name

    def run(self):
        print(f"{self.name}开始...")
        time.sleep(3)
        print(f"{self.name}结束...")

t = MyThread("任务")
t.start()
```

### tcp 并发

在网络编程章节，实现了一个聊天室的案例，但是仅仅支持服务端和客户端的一对一通讯，现在就可以通过开辟多个线程让服务端支持与多个客户端进行通信。

::: code-group

```py [server.py]
import socket
from threading import Thread

server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server.bind(("127.0.0.1", 3002))
server.listen(5)

def task(conn):
    while True:
        try:
            data = conn.recv(1024)
        except Exception:
            break

        if not data:
            break

        print(data.decode("utf-8"))
        conn.send(data.upper())

while True:
    conn, addr = server.accept()
    # 每次收发都会开辟一个新的线程用来服务不同的客户
    # 除了线程模块，进程同样也可以去做这个事情
    t = Process(target=task, args=(conn,))
    t.start()

```

```py [client.py]
import socket
import time

client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
client.connect(("127.0.0.1", 3002))

while True:
    client.send(b'hello')
    data = client.recv(1024)
    print(data.decode("utf-8"))
    time.sleep(2)
```

:::

### 守护线程、线程数据共享

首先确认的一点是，在线程是在同一个进程下开设的。验证方法如下： 通过运行后的打印信息可以看出，他们的进程 ID 都是同一个。

```py
from threading import Thread
import os
import time

def task():
    print("任务开始")
    time.sleep(2)
    print(f"任务结束, 子线程 {os.getpid()}")

if __name__ == '__main__':
    t = Thread(target=task)
    t.start()
    t.join()  # 等待线程执行执行完毕后，在往后执行
    print(f"主进程{os.getpid()}")

# 任务开始
# 任务结束, 子线程 32626
# 主进程32626
```

然后可以确认线程的数量和线程的名称，示例代码如下：

```py
from threading import Thread, current_thread, active_count
import time


def fn1():
    time.sleep(1)
    print('任务一', current_thread().name)


def fn2():
    time.sleep(1)
    print('任务二', current_thread().name)


t1 = Thread(target=fn1)
t2 = Thread(target=fn2)
t1.start()
t2.start()

thread = current_thread()
print('主线程任务', thread.name)
print('线程统计', active_count())

# 主线程任务 MainThread
# 线程统计 3  需要注意一定如果你没有加 sleep 可以打印的数量不是很准确，因为开设线程不是很消耗资源，所以速度会非常快
# 任务一 Thread-1 (fn1)
# 任务二 Thread-2 (fn2)
```

可以看出有主线程和子线程，守护线程就是如果主线程执行完毕，就要销毁对应的子线程。让子线程变成守护线程即可。

```py
from threading import Thread, current_thread
import time

def task():
    print(f"{current_thread().name} 还活着")
    time.sleep(2)
    print(f"{current_thread().name} 死亡")

if __name__ == '__main__':
    t = Thread(target=task)
    # 如果不加这个代码，把子线程变成守护线程 Thread-1 (task) 死亡 就会被打印
    # 如果将 Thread-1 (task) 设为守护线程，死亡就不给被打印
    t.daemon = True
    t.start()

    print(f"{current_thread().name} 死亡")

# Thread-1 (task) 还活着
# MainThread 死亡
# Thread-1 (task) 死亡
```

关于守护进程的执行顺序思考：

```py
from threading import Thread, current_thread
import time

def fn1():
    print(f"{current_thread().name}, 开始执行")
    time.sleep(1)
    print(f"{current_thread().name}, 执行结束")


def fn2():
    print(f"{current_thread().name}, 开始执行")
    time.sleep(2)
    print(f"{current_thread().name}, 执行结束")

t1 = Thread(target=fn1)
t2 = Thread(target=fn2)
t1.daemon = True
t1.start()
t2.start()

print(f"{current_thread().name}, 执行结束")

# 打印结果分析：
# 主线程会等待子线程执行完毕后，回收子线程的资源，虽然把 t1 设为守护线程，但是 t2 结束之前主线程是不会被销毁的。

# Thread-1 (fn1), 开始执行
# Thread-2 (fn2), 开始执行
# MainThread, 执行结束
# Thread-1 (fn1), 执行结束
# Thread-2 (fn2), 执行结束
```

### 互斥锁

在线程中同样存在抢数据的情况，遇到这种情况解决的方法便是互斥锁。示例代码如下：

```py
from threading import Thread
import time

total = 200
def decrement():
    global total
    temp = total
    time.sleep(0.05)
    total = temp - 1

if __name__ == '__main__':
    t_list = []
    for i in range(1, 181):
        t = Thread(target=decrement)
        t.start()
        t_list.append(t)

    for i in t_list:
        i.join()

    print(total) # 199

# 每次开启一个新的线程，都是操作
# temp = total
# time.sleep(0.05)
# total = temp - 1
# 只会导致最终结果为 199
```

给程序加锁后，每次让线程去抢锁，程序运行结束之后再释放锁，让并发的执行变成串行，虽然执行效率会变慢，但是保证了数据的一致性。示例代码如下：

```py
from threading import Thread, Lock
import time

mutex = Lock() # 由于线程中的数据是可以共享的，所以可以直接声明
total = 200
def decrement():
    global total
    mutex.acquire() # 加锁
    temp = total
    time.sleep(0.05)
    total = temp - 1
    metxt.release() # 释放锁

    # 当然还有更简单的写法, 相当于以上的两步
    with mutex:
        temp = total
        time.sleep(0.05)
        total = temp - 1

if __name__ == '__main__':
    t_list = []
    for i in range(1, 181):
        t = Thread(target=decrement)
        t.start()
        t_list.append(t)

    for i in t_list:
        i.join()

    print(total) # 20
```

### GIL 全局解释器锁

::: warning GIL (Global Interperter Lock) 是 CPython 的特点并不是 Python 的特点。

在 CPython 解释其中，GIL 是一把互斥锁（用来保护 Python 对象的访问），用来阻止同一个进程下多个线程同时执行，也就是说同一个进程下多个线程没法实现并行，有多个 CPU 都不能并行，一次只有一个 CPU 执行。

这么看来 Python 的多线程好像用处并不大，无法利用多核优势，一次只能使用一个核。

这个互斥锁的存在也是有一定必要的，因为 Python 的**内存管理**不是线程安全的。 如图：

GIL 会导致一个进程下的多个线程无法同时执行，无法利用多核的能力

GIL 保证的是解释器级别的数据安全

![GIL](/py/GIL.png)
:::

#### 内存管理（垃圾回收）

::: warning 内存管理

- 引用计数：每个对象都有一个引用计数，表示指向该对象的引用数量，当一个对象的引用计数减少到零时，Python 会自动将其释放，并回收其占用的内存。但是引用计数无法解决循环引用的问题。循环引用检测：Python 的垃圾回收器还包括一个循环引用检测器，用于检测并解决循环引用的情况，垃圾回收器通过定期检查对象图来识别循环引用，并在必要时释放相关对象的内存。

- 标记清除：标记清除分为两个阶段，标记阶段垃圾回收器从一组根对象开始递归遍历多有可达对象标记为活动对象。清除阶段是标记结束后，遍历整个内存空间找到未被标记的对象将它们的内存进行释放。

- 分代回收：分代回收是一种优化垃圾回收性能的策略，将对象分成不同的代别，主要分为年轻代和老年代，年轻代通常采用较小的内存空间，并且垃圾回收效率比较高，迅速回收不再使用的对象。当对象在年轻代中经过了多次垃圾回收后，会被提升为老年代，老年代通常采用较大的内存空间，垃圾回收不太频繁。

:::
`gc（garbage collector）`模块，可以用于控制和配置垃圾回收机制。虽然大多数情况下你不需要手动干预垃圾回收，但有时你可能需要在特殊情况下手动执行垃圾回收。

1. `gc.collect([generation])`: 手动触发垃圾回收。可以指定代数（generation），默认为 2。
2. `gc.get_count()`: 返回每个代的对象计数，一般为(0, 0, 0)，(0, 0, 1)，或(0, 1, 0)。
3. `gc.get_stats()`: 返回一个包含有关垃圾回收的统计信息的列表。
4. `gc.set_debug(flags)`: 设置垃圾回收调试标志。
5. `gc.get_objects()`: 返回当前 Python 进程中的所有对象列表。

#### 验证 GIL 的存在

```py
from threading import Thread

total = 180

def decrement():
    global total
    temp = total
    total = temp - 1

if __name__ == '__main__':
    t_list = []
    for i in range(180):
        t = Thread(target=decrement)
        t.start()
        t_list.append(t)

    for i in t_list:
        i.join()

    print(total) # 0
```

同样的代码去掉了，`sleep` 结果就正确了？原因就是在程序的一开始大家都会去抢 GIL 锁，第一个拿到锁的的线程就会执行代码，然后释放锁。以此类推 180 个线程结束后，结果就是 0 了

如果带上 `sleep`，每次到这行代码后就会释放锁，等同于 180 个想成都拿到了 total 的值是 180，然后在都去执行 180 - 1，当然如果把这个 `sleep` 当做网络延迟，目前是无法去除的，所以只能自己加锁。

#### Python 的多线程适合什么事情？

由于 GIL 的存在，多线程并不会实现并发，表面看多线程的优势好像是没了，但这并不是完全正确的，具体还要通过场景来看：

- `I/O` 密集型任务：比如网络请求、文件读写、数据库查询等操作，使用多线程比较优势。线程之间的切换开销相对比较小。
- `CPU` 密集型任务：比如科学计算、图像处理、数据分析等，多进程能够充分利用多核 `CPU` 的优势。提高计算新能。

`CPU` 密集型测试代码如下：总结得出结论，纯计算型任务，多进程的优势还是很大的。

```py
import os
import time
from threading import Thread
from multiprocessing import Process

def task():
    total = 0
    for i in range(100000000):
        total += i

if __name__ == '__main__':
    start = time.time()
    arr = []
    # os.cpu_count 获取电脑的核心数，此电脑为 8 核
    # 8 个进程和 8 个线程的效率差距
    for i in range(os.cpu_count()):
        # t = Thread(target=task)  # 19.162802934646606
        t = Process(target=task)  # 6.387197017669678
        t.start()
        arr.append(t)

    for i in arr:
        i.join()
    end = time.time()
    print(end - start)
```

`I/O` 密集型测试代码如下：

```py
import time
from threading import Thread
from multiprocessing import Process

def task():
    time.sleep(1)

if __name__ == '__main__':
    start = time.time()
    arr = []
    for i in range(2000):
        # 2000 个线程和 2000 个进程的效率差距
        # t = Thread(target=task)  # 1.180966854095459
        t = Process(target=task)  # 17.913624048233032
        t.start()
        arr.append(t)

    for i in arr:
        i.join()
    end = time.time()
    print(end - start)
```
