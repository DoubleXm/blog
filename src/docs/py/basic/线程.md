::: tip
在 Python 中进程和线程都是用于多任务处理的重要概念。它们允许程序同时执行多个任务，但是会有一些区别。

- 进程：进程是操作系统分配资源的基本单位，每个进程都有自己独立的内存空间和系统资源。进程之间相互独立，数据不互通，如果想要数据互通需要利用 （Inter-Process Communication，IPC）机制。如管道、队列。

- 线程：线程是进程内的执行单元，因此进程中的线程是数据共享的，但是需要注意线程安全问题，比如可以使用锁机制。

总结：进程适合需要隔离和并行执行的任务，线程适合数据共享的任务。**需要注意的是由于 Python 中的全局解释锁（Global Interpreter Lock，GIL）限制，多线程在 CPU 密集型任务中可能无法实现真正的并行执行，但对于 I/O 密集型任务是可以的。如果的确需要并行可以考虑使用多进程。**
:::

## 进程

创建进程的方式是通过 `multiprocessing` 模块下的 `Process`，该方法接受两个参数，一个新进程需要运行的函数和一个可选参数（元祖类型，最少两个）。

- `start`: 启动进程
- `join`: 等待进程执行完成

`current_process` 则是获取当前正在运行的进程信息。

```py
from multiprocessing import Process, current_process
import os

def task(name):
    process = current_process()
    # 步骤一：子进程的 id: 63888, 子进程父 id: 63886
    print(f"子进程的 id: {process.pid}, 子进程父 id: {os.getppid()}")

if __name__ == "__main__":
    p = Process(target=task, args=("子进程",))
    p.start()
    p.join()
    # 步骤二：主进程 id, 63886
    print(f"主进程 id, {os.getpid()}")
```

### join 方法

此方法是等待被开启的进程执行完毕，再往下执行。示例如下：

```py
from multiprocessing import Process
import time

def task():
    print("任务开始执行。。。")
    time.sleep(1)
    print("任务执行结束。。。")

if __name__ == "__main__":
    p = Process(target=task)
    p.start()

    print("主进程执行完毕")

# 可以看出打印结果是先执行主进程，然后再去打印子进程的代码
# 主进程执行完毕
# 任务开始执行。。。
# 任务执行结束。。。

# 如果在 start 后加入 join 方法打印结果则是
# 任务开始执行。。。
# 任务执行结束。。。
# 主进程执行完毕
```

### 僵尸进程 Zombie Process

子进程死后还有有一些资源占用（进程号，进程运行状态，进程运行时间等），等待父进程调用系统进行资源回收，除了 `init`（系统初始化进程） 进程之外，所有的进程最后都会步入僵尸进程。

危害：子进程退出后，父进程没有及时处理，僵尸进程就会占用系统资源，如果产生了大量的僵尸进程，系统资源占用过度，系统没有可用的进程号，导致系统不能产生新的进程。

### 孤儿进程 Orphan Process

孤儿进程值父进程意外或被提前终止，导致子进程成为孤儿，没有父进程，当一个进程成为孤儿进程时，会被 `init` 进程接受，成为 `init` 的子进程。由 `init` 进程负责孤儿进程的资源清理和回收，保证其能够正常被终止。

### 守护进程 Daemon Process

守护进程是后台运行的进程，守护进程的目的是执行系统服务，定时任务等，通常在系统启动式就启动并持续运行。

```py
from multiprocessing import Process
import time

def task():
    print("任务开始...")
    time.sleep(3)
    print("任务结束...")

if __name__ == "__main__":
    p = Process(target=task)
    # daemo 开启守护进程
    p.daemon = True
    p.start()

    print("主进程执行完毕...")

# 当守护进程开启后，只要主进程执行完毕，子进程也会跟着结束。所以后续的内容就不会再被打印
# 主进程执行完毕...
```

### 并发 Concurrency 、并行 Parallelism

- 并发：多个进程或者线程在同一时间段内执行，并且相互交替执行，并发并不意味着一起执行，因为多个进程和线程可能在单个处理器上交替执行。
- 并行：多个进程或者线程在同一时间内执行，需要用到处理器的多核能力，并行可以显著提高程序的运行速度

### 互斥锁

当多个进程操作同一份数据的时候，就会引发数据错乱的问题，解决的方法就是加锁处理，**锁的原理就是把并发变成串行，虽然牺牲了效率，但是保证了数据的安全性**

假设有一个抢票平台，目前余票剩余两张，但是有 8 个用户去抢这个余票，代码可能是如下这样：

::: code-group

```py [main.py]
from multiprocessing import Process
import time
import random
import json

# 查询余票
def query_ticket(name):
    with open("./tickets.json", "r", encoding="utf8") as f:
        dic = json.load(f)
        print(f"{name} 查询余票数量为 {dic['ticket']}")

def buy_ticket(name):
    with open("./tickets.json", "r", encoding="utf8") as f:
        dic = json.load(f)

    # 模拟网络延迟
    time.sleep(random.randint(1, 3))
    if dic["ticket"] > 0:
        dic["ticket"] -= 1
        with open("./tickets.json", "w", encoding="utf-8") as f:
            json.dump(dic, f)
        print(f"{name} 购买一张余票，余票数量为 {dic['ticket']}")
    else:
        print(f"余票不足，{name}买票失败")

def common(name):
    query_ticket(name)
    buy_ticket(name)

if __name__ == "__main__":
    for i in range(1, 9):
        p = Process(target=common, args=(f"用户{i} ",))
        p.start()

```

```json [tickets.json]
{ "ticket": 2 }
```

:::

代码执行后，可以看出所有的用户都能买到票。并且余票是 1。原因就是多进程执行时都触发了购买，读到了同一个数据源，导致了数据的错乱，解决的办法就是加一把锁，谁先抢到这把锁，谁就先开始买票，买票之后再把锁释放，让其余的进程继续抢这个锁。修复后的代码如下

```py
from multiprocessing import Process, Lock, set_start_method
import json
import time
import random

def query_ticket(name):
    with open("./tickets.json", "r", encoding="utf-8") as f:
        dic = json.load(f)

    print(f"{name} 查询余票，余票数量为：{dic['ticket_num']}")

def buy_ticket(name):
    with open("./tickets.json", "r", encoding="utf-8") as f:
        dic = json.load(f)

    # 模拟网络延迟
    time.sleep(random.randint(1, 5))
    if dic["ticket_num"] > 0:
        dic["ticket_num"] -= 1
        with open("./tickets.json", "w", encoding="utf-8") as f:
            json.dump(dic, f)
        print(f"{name} 购买成功，余票数量为：{dic['ticket_num']}")
    else:
        print(f"{name} 购票失败，余票不足")

def task(name, mutex):
    query_ticket(name)

    # 给购买的过程加锁
    mutex.acquire()
    buy_ticket(name)
    # 购买之后释放锁
    mutex.release()

if __name__ == "__main__":
    # mac 平台需要次代码不然会报错
    set_start_method('fork')
    mutex = Lock()
    for i in range(1, 9):
        p = Process(target=task, args=(f"用户{i} ", mutex))
        p.start()
```

为什么 `mac` 要加 `set_start_method('fork')` 原因如下：

::: warning
python3.4 更新后，默认用“spawn”，开启进程，我们要主动指定为“fork”

- spawn：使用此方式启动的进程，只会执行和 target 参数或者 run() 方法相关的代码。Windows 平台只能使用此方法，事实上该平台默认使用的也是该启动方式。相比其他两种方式，此方式启动进程的效率最低。
- fork：使用此方式启动的进程，基本等同于主进程（即主进程拥有的资源，该子进程全都有）。因此，该子进程会从创建位置起，和主进程一样执行程序中的代码。注意，此启动方式仅适用于 UNIX 平台，os.fork() 创建的进程就是采用此方式启动的。
- forserver：使用此方式，程序将会启动一个服务器进程。即当程序每次请求启动新进程时，父进程都会连接到该服务器进程，请求由服务器进程来创建新进程。通过这种方式启动的进程不需要从父进程继承资源。注意，此启动方式只在 UNIX 平台上有效。

:::

### 队列 Queue

队列的特性就是先进先出，当然 Py 中还有先进后出的队列使用 `from queue import LifoQueue` 方法与该队列基本一致。

```py
from multiprocessing import Queue

if __name__ == "__main__":
    queue = Queue(maxsize=3)
    queue.put(1)
    queue.put(2)
    queue.put(3)
    # 当超过队列长度时，会阻塞程序的运行，可以传入timeout 参数或者使用 put_nowait 方法
    # queue.put_nowait(4)
    # queue.put(4, timeout=3)

    print(queue.get())
    print(queue.get())
    print(queue.get())
    # 当取值超过队列长度时仍然会阻塞程序的执行, 可以传入 timeout 参数或者使用 get_nowait 方法  # noqa: E501
    # print(queue.get(timeout=2))
    # queue.get_nowait()

    # 判断是否为空队列
    print(queue.empty())
    # 判断队列是否是满的
    print(queue.full())
```

### 进程通信 IPC

IPC 是 Inter-Process Communication（进程间通信）的缩写，它是一种允许不同进程之间进行数据交换和通信的机制。在计算机科学和操作系统领域，IPC 是一种重要的概念，它允许多个进程在同一台计算机上协同工作、共享资源和协调任务。

```py
from multiprocessing import Process, Queue, current_process, set_start_method

def task1(q):
    process = current_process()
    q.put(f"子进程 {process.pid}存的数据")

def task2(q):
    process = current_process()
    # 子进程 84474 取数据：子进程 84473存的数据
    print(f"子进程 {process.pid} 取数据：{q.get()}")

if __name__ == "__main__":
    set_start_method("fork")
    queue = Queue()
    p1 = Process(target=task1, args=(queue,))
    p2 = Process(target=task2, args=(queue,))
    # 开启两个子进程
    p1.start()
    p2.start()

```

![process1](/py/process1.png)

#### 生产者消费者模型

生产者：负责生产和制造数据，消费者：负责消费和处理数据。通过以下代码创建出两个生产者（厨子）和两个消费者（吃货），但是还存在一些问题。比如消费者的内部是一个 `while` 循环，当消费者消费完生产者的数据时，如何跳过这个循环？

```py
from multiprocessing import Process, Queue, set_start_method
import time
import random


def producer(name, food, q):
    for i in range(10):
        # 模拟生产时间
        time.sleep(random.randint(1, 3))
        q.put(f"{food}{i}")
        print(f"{name} 生产了 {food}{i}")

def consumer(name, q):
    while True:
        # 模拟消费时间
        time.sleep(random.randint(1, 3))
        food = q.get()
        print(f"{name} 消费了 {food}")

if __name__ == "__main__":
    set_start_method("fork")
    queue = Queue()
    p1 = Process(target=producer, args=("神厨小福贵", "沙琪玛", queue))
    p2 = Process(target=producer, args=("中华小当家", "麻婆豆腐", queue))
    c1 = Process(target=consumer, args=("八戒", queue))
    c2 = Process(target=consumer, args=("福迪", queue))

    p1.start()
    p2.start()
    c1.start()
    c2.start()
```

如果想要跳出这个循环，我们可以在队列中增加两条数据作为标识。如下：

```py
# 改造消费者代码
def consumer(name, q):
    while True:
        # 模拟消费时间
        time.sleep(random.randint(1, 3))
        data = q.get()
        print(f"{name} 消费了 {data}")
        if data == "厨子下班了":
            break

if __name__ == "__main__":
    # ...

    # 等到 p1, p2 执行完毕后，在往队列中 put 两条标识数据
    p1.join()
    p2.join()
    queue.put("厨子下班了")
    queue.put("厨子下班了")

```

但还是要考虑一个问题，如果有 n 个生产者，那代码写的太啰嗦了（当然你可能会通过元类或者继承的方式，来统计消费者的个数，然后去循环添加）。

解决方案可以使用另一个对象 `JoinableQueue`，相比于 `Queue` 的基础之上增加了一个计数器的功能，每 `put` 一个数据计数器就 +1，每调用一个

`task_done` 计数器就 -1。当计数器为 0 时会执行 `q.join()` 后面的代码。

```py
from multiprocessing import Process, set_start_method, JoinableQueue
import time
import random

def producer(name, food, q):
    for i in range(10):
        # 模拟生产时间
        time.sleep(random.randint(1, 3))
        q.put(f"{food}{i}")
        print(f"{name} 生产了 {food}{i}")

def consumer(name, q):
    while True:
        # 模拟消费时间
        time.sleep(random.randint(1, 3))
        data = q.get()
        print(f"{name} 消费了 {data}")
        # 告诉队列已经取出了一条数据，并且处理完毕
        q.task_done()

if __name__ == "__main__":
    set_start_method("fork")
    queue = JoinableQueue()
    p1 = Process(target=producer, args=("神厨小福贵", "沙琪玛", queue))
    p2 = Process(target=producer, args=("中华小当家", "麻婆豆腐", queue))
    c1 = Process(target=consumer, args=("八戒", queue))
    c2 = Process(target=consumer, args=("福迪", queue))

    p1.start()
    p2.start()

    # 设置子进程为主进程的守护进程
    c1.daemon = True
    c2.daemon = True
    c1.start()
    c2.start()

    # 防止生产者数据生产慢计数器为 0 的情况提前发生，join 还是需要保留
    p1.join()
    p2.join()

    # 执行到这里时，主进程已经执行完毕，队列也已经是空的，此时消费者已经没有了存在的意义
    # 并且主进程已经结束，可以讲子进程设置成守护进程，即可实现子进程的退出
    queue.join()  # 等待队列中所有数据被取完

```

## 线程
